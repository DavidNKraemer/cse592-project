\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\abx@aux@refcontext{ynt/global//global/global}
\abx@aux@cite{sdlbfgs}
\abx@aux@segm{0}{0}{sdlbfgs}
\abx@aux@cite{tensorflow}
\abx@aux@segm{0}{0}{tensorflow}
\@writefile{toc}{\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax }
\@writefile{lof}{\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax }
\@writefile{lot}{\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\abx@aux@segm{0}{0}{sdlbfgs}
\abx@aux@segm{0}{0}{sdlbfgs}
\abx@aux@segm{0}{0}{sdlbfgs}
\abx@aux@cite{boyd}
\abx@aux@segm{0}{0}{boyd}
\abx@aux@segm{0}{0}{sdlbfgs}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {2}Review of theory}{2}{section.2}}
\newlabel{prb:mini}{{1}{2}{Review of theory}{equation.2.1}{}}
\newlabel{alg:sqn}{{2}{3}{Review of theory}{Item.6}{}}
\@writefile{loa}{\defcounter {refsection}{0}\relax }\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces The high level stochastic quasi-Newton minimization algorithm. Given an initial point $x_1 \in \mathbb  {R}^n$, batch sizes $\ip  {m_k}$, step sizes $\ip  {\alpha _k}$, and a maximum memory capacity $p$, perform BFGS with the stochastic dampened direction update. }}{3}{algorithm.1}}
\@writefile{loa}{\defcounter {refsection}{0}\relax }\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces The SdLBFGS update step. The resulting output is $H_k g_k = v_p$, where $H_k$ is the approximation of the $k$th iterate Hessian and $g_k$ is the approximation of the $k$th iterate gradient. Note that the computation of $H_k$ is implicit, preventing additional storage requirements. }}{3}{algorithm.2}}
\newlabel{alg:sdlbfgs}{{2}{3}{Review of theory}{algorithm.2}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {3}Implementation}{3}{section.3}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {4}Empirical results}{3}{section.4}}
\abx@aux@refcontextdefaultsdone
\abx@aux@defaultrefcontext{0}{boyd}{ynt/global//global/global}
\abx@aux@defaultrefcontext{0}{tensorflow}{ynt/global//global/global}
\abx@aux@defaultrefcontext{0}{sdlbfgs}{ynt/global//global/global}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {5}Discussion}{4}{section.5}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{References}{4}{section*.1}}
