\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\abx@aux@refcontext{ynt/global//global/global}
\abx@aux@cite{sdlbfgs}
\abx@aux@segm{0}{0}{sdlbfgs}
\abx@aux@cite{tensorflow}
\abx@aux@segm{0}{0}{tensorflow}
\@writefile{toc}{\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax }
\@writefile{lof}{\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax }
\@writefile{lot}{\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\abx@aux@segm{0}{0}{sdlbfgs}
\abx@aux@segm{0}{0}{sdlbfgs}
\abx@aux@segm{0}{0}{sdlbfgs}
\abx@aux@cite{boyd}
\abx@aux@segm{0}{0}{boyd}
\abx@aux@segm{0}{0}{sdlbfgs}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {2}Review of theory}{2}{section.2}}
\newlabel{prb:mini}{{1}{2}{Review of theory}{equation.2.1}{}}
\abx@aux@cite{numpy}
\abx@aux@segm{0}{0}{numpy}
\newlabel{alg:sqn}{{2}{3}{Review of theory}{Item.6}{}}
\@writefile{loa}{\defcounter {refsection}{0}\relax }\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces The high level stochastic quasi-Newton minimization algorithm. Given an initial point $x_1 \in \mathbb  {R}^n$, batch sizes $\ip  {m_k}$, step sizes $\ip  {\alpha _k}$, and a maximum memory capacity $p$, perform BFGS with the stochastic dampened direction update. }}{3}{algorithm.1}}
\@writefile{loa}{\defcounter {refsection}{0}\relax }\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces The SdLBFGS update step. The resulting output is $H_k g_k = v_p$, where $H_k$ is the approximation of the $k$th iterate Hessian and $g_k$ is the approximation of the $k$th iterate gradient. Note that the computation of $H_k$ is implicit, preventing additional storage requirements. }}{3}{algorithm.2}}
\newlabel{alg:sdlbfgs}{{2}{3}{Review of theory}{algorithm.2}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {3}Implementation}{3}{section.3}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces The typical workflow for using built-in \texttt  {Optimizer} objects in a statistical learning setting. Here the \texttt  {optimizer} is an operation which is configured to minimize the objective function. The actual minimization occurs inside of the session block.}}{4}{figure.1}}
\newlabel{fig:typical}{{1}{4}{The typical workflow for using built-in \texttt {Optimizer} objects in a statistical learning setting. Here the \texttt {optimizer} is an operation which is configured to minimize the objective function. The actual minimization occurs inside of the session block}{figure.1}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces The workflow for using objects of \texttt  {ExternalOptimizerInterface} subclasses, such as the implementation of \texttt  {SQNOptimizer}. Here the \texttt  {optimizer} itself is the minimizer while the \texttt  {minimize} method actually performs estimation of the minimal objective function value. }}{4}{figure.2}}
\newlabel{fig:external}{{2}{4}{The workflow for using objects of \texttt {ExternalOptimizerInterface} subclasses, such as the implementation of \texttt {SQNOptimizer}. Here the \texttt {optimizer} itself is the minimizer while the \texttt {minimize} method actually performs estimation of the minimal objective function value}{figure.2}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {4}Empirical results}{4}{section.4}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {5}Discussion}{4}{section.5}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{References}{4}{section*.1}}
\abx@aux@refcontextdefaultsdone
\abx@aux@defaultrefcontext{0}{boyd}{ynt/global//global/global}
\abx@aux@defaultrefcontext{0}{numpy}{ynt/global//global/global}
\abx@aux@defaultrefcontext{0}{tensorflow}{ynt/global//global/global}
\abx@aux@defaultrefcontext{0}{sdlbfgs}{ynt/global//global/global}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces  SdLBFGS compared with traditional BFGS together with AdaGrad and Stochastic Gradient Descent on the problem $f(x) = x^6 - 2x^5 + x^3 - x^2 + 3x$, shown on the right. Each algorithm is initialized near the local minimum at around $x = 1.6$. We see that SGD, AdaGrad, and BFGS quickly converge to the local minimum, whereas SdLBFGS converges to the global minimum at around $x = -0.8$ more slowly. }}{5}{figure.3}}
\newlabel{fig:convex-linear-system}{{3}{5}{SdLBFGS compared with traditional BFGS together with AdaGrad and Stochastic Gradient Descent on the problem $f(x) = x^6 - 2x^5 + x^3 - x^2 + 3x$, shown on the right. Each algorithm is initialized near the local minimum at around $x = 1.6$. We see that SGD, AdaGrad, and BFGS quickly converge to the local minimum, whereas SdLBFGS converges to the global minimum at around $x = -0.8$ more slowly}{figure.3}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces  SdLBFGS performance on the solving the overdetermined linear system $Ax = b$ with $A \in \mathbb  {R}^{50 \times 100}$ with convex smooth loss function $\norm  {Ax-b}_2^2$. We sweep different memory capacity values $p \in \set  {1,10,20,30}$ and observe that convergence is approximately equivalent between configurations. }}{5}{figure.4}}
\newlabel{fig:convex-linear-system}{{4}{5}{SdLBFGS performance on the solving the overdetermined linear system $Ax = b$ with $A \in \RR ^{50 \times 100}$ with convex smooth loss function $\norm {Ax-b}_2^2$. We sweep different memory capacity values $p \in \set {1,10,20,30}$ and observe that convergence is approximately equivalent between configurations}{figure.4}{}}
